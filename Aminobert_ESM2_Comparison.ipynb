{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vksriharsha/Aminobert_ESM2_Comparative_Study/blob/main/Aminobert_ESM2_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Q9RSy46tE1"
      },
      "source": [
        "#Import libraries for ESM-2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JCtdfHxtfWNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01493983-8834-41e9-ec42-0f78c2f14220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M5U4p9JdmKag",
        "outputId": "dd610db2-2440-4551-f276-16ecc78b65d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fair-esm[esmfold] in /usr/local/lib/python3.8/dist-packages (2.0.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from fair-esm[esmfold]) (0.1.8)\n",
            "Collecting deepspeed==0.5.9\n",
            "  Downloading deepspeed-0.5.9.tar.gz (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.3/510.3 KB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 KB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.80-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fair-esm[esmfold]) (1.7.3)\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (21.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (5.4.8)\n",
            "Collecting py-cpuinfo\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (1.13.0+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (4.64.1)\n",
            "Collecting triton==1.0.0\n",
            "  Downloading triton-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from ml-collections->fair-esm[esmfold]) (1.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from ml-collections->fair-esm[esmfold]) (6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from ml-collections->fair-esm[esmfold]) (1.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from ml-collections->fair-esm[esmfold]) (0.5.5)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->fair-esm[esmfold]) (4.4.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->fair-esm[esmfold]) (2022.11.0)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (3.8.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->deepspeed==0.5.9->fair-esm[esmfold]) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning->fair-esm[esmfold]) (3.19.6)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (22.2.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->fair-esm[esmfold]) (1.24.3)\n",
            "Building wheels for collected packages: deepspeed, ml-collections, antlr4-python3-runtime\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.5.9-py3-none-any.whl size=524336 sha256=a5ead6eda9941b306eb2beb6faf4d206b262ab937a2e6e7295a880b3a7692400\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/41/fc/116ba18dcc5a75476d24b65604a2faaa21c0be0f5784f3ef25\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=cbbb48f20d61f501f877397618c6ca988fa33da6ca090281244ba9a4affb6430\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/9f/a9/9e8309035a5bf09ed9086bbca8c9b74cb6413d3eb203e2bc8c\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=ff10148fe695db95297a549d621c9c4743be7b85652b0d62d1a11aa9ed206f1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
            "Successfully built deepspeed ml-collections antlr4-python3-runtime\n",
            "Installing collected packages: py-cpuinfo, ninja, hjson, antlr4-python3-runtime, tensorboardX, omegaconf, ml-collections, einops, biopython, triton, torchmetrics, lightning-utilities, deepspeed, pytorch-lightning\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 biopython-1.80 deepspeed-0.5.9 einops-0.6.0 hjson-3.1.0 lightning-utilities-0.5.0 ml-collections-0.1.1 ninja-1.11.1 omegaconf-2.3.0 py-cpuinfo-9.0.0 pytorch-lightning-1.8.6 tensorboardX-2.5.1 torchmetrics-0.11.0 triton-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Bio\n",
            "  Downloading bio-1.5.3-py3-none-any.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.6/272.6 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from Bio) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from Bio) (4.64.1)\n",
            "Requirement already satisfied: biopython>=1.80 in /usr/local/lib/python3.8/dist-packages (from Bio) (1.80)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from biopython>=1.80->Bio) (1.21.6)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (2022.12.7)\n",
            "Installing collected packages: biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.5.3 biothings-client-0.2.6 mygene-3.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.8/dist-packages (1.80)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from biopython) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dllogger@ git+https://github.com/NVIDIA/dllogger.git\n",
            "  Cloning https://github.com/NVIDIA/dllogger.git to /tmp/pip-install-0_nwjirh/dllogger_734fa37a79084761be953eeec81a8bd1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/dllogger.git /tmp/pip-install-0_nwjirh/dllogger_734fa37a79084761be953eeec81a8bd1\n",
            "  Resolved https://github.com/NVIDIA/dllogger.git to commit 0540a43971f4a8a16693a9de9de73c1072020769\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: dllogger\n",
            "  Building wheel for dllogger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dllogger: filename=DLLogger-1.0.0-py3-none-any.whl size=5670 sha256=986286bc80071e789f23735f59a4c97470a524861e5d04409cc245416402c393\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2n8uuxim/wheels/c4/f9/e7/05e0371c5078725c26b6251a30b873f061a4387de03d3fcbcb\n",
            "Successfully built dllogger\n",
            "Installing collected packages: dllogger\n",
            "Successfully installed dllogger-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openfold@ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307\n",
            "  Cloning https://github.com/aqlaboratory/openfold.git (to revision 4b41059694619831a7db195b7e0988fc4ff3a307) to /tmp/pip-install-pishx68v/openfold_96b19cb00ecd47d797e6079cce443f8a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/aqlaboratory/openfold.git /tmp/pip-install-pishx68v/openfold_96b19cb00ecd47d797e6079cce443f8a\n",
            "  Running command git rev-parse -q --verify 'sha^4b41059694619831a7db195b7e0988fc4ff3a307'\n",
            "  Running command git fetch -q https://github.com/aqlaboratory/openfold.git 4b41059694619831a7db195b7e0988fc4ff3a307\n",
            "  Running command git checkout -q 4b41059694619831a7db195b7e0988fc4ff3a307\n",
            "  Resolved https://github.com/aqlaboratory/openfold.git to commit 4b41059694619831a7db195b7e0988fc4ff3a307\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: openfold\n",
            "  Building wheel for openfold (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openfold: filename=openfold-1.0.0-cp38-cp38-linux_x86_64.whl size=2566635 sha256=0f05daf74f024d298c36a4b6a6f5f6b26df08111d0dbada9f8f46ee02aeb1f23\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/e4/59/4ca275bd404099dc825479b6f77e81b8443afbc04f960ce616\n",
            "Successfully built openfold\n",
            "Installing collected packages: openfold\n",
            "Successfully installed openfold-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/esm.git\n",
            "  Cloning https://github.com/facebookresearch/esm.git to /tmp/pip-req-build-mzw7202n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/esm.git /tmp/pip-req-build-mzw7202n\n",
            "  Resolved https://github.com/facebookresearch/esm.git to commit 7c2beef1eb74d8b5744f28ffc215a244d874a74f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fair-esm\n",
            "  Building wheel for fair-esm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fair-esm: filename=fair_esm-2.0.1-py3-none-any.whl size=96091 sha256=2ef31d9248bede50143b0691249cd7133f64aead434d690cab2632ff6b9e314f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zf9g3lus/wheels/4d/93/84/94f97c0c2eb9e55cec3d98446534316a88262b407db4c247e7\n",
            "Successfully built fair-esm\n",
            "Installing collected packages: fair-esm\n",
            "  Attempting uninstall: fair-esm\n",
            "    Found existing installation: fair-esm 2.0.0\n",
            "    Uninstalling fair-esm-2.0.0:\n",
            "      Successfully uninstalled fair-esm-2.0.0\n",
            "Successfully installed fair-esm-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install fair-esm\n",
        "!pip install fair-esm[esmfold]\n",
        "!pip install Bio\n",
        "!pip install biopython\n",
        "# OpenFold and its remaining dependency\n",
        "!pip install 'dllogger @ git+https://github.com/NVIDIA/dllogger.git'\n",
        "!pip install 'openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307'\n",
        "!pip install git+https://github.com/facebookresearch/esm.git  # bleeding edge, current repo main branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wNnIORRpls4C"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import torch\n",
        "import esm\n",
        "import os\n",
        "import re\n",
        "from ctypes import sizeof\n",
        "from collections import OrderedDict \n",
        "import sys\n",
        "import shutil\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-zbmfIafnGV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9a9635-6c5e-410a-88f5-ac9dcec26036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/esm/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
          ]
        }
      ],
      "source": [
        "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8U9GfijSFiX"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OpJwQseTSnv"
      },
      "source": [
        "## Set folder path of the PDB files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2T-wvgxFTRAd"
      },
      "outputs": [],
      "source": [
        "# Folder Path\n",
        "path = r\"/content/drive/MyDrive/PDB\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSPOeuPfRIU7"
      },
      "source": [
        "## Function to convert PDB to FASTA file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a4rNDMJpCQON"
      },
      "outputs": [],
      "source": [
        "def convertPDBtoFASTA(pdb_file):\n",
        "    aa3to1={\n",
        "       'ALA':'A', 'VAL':'V', 'PHE':'F', 'PRO':'P', 'MET':'M',\n",
        "       'ILE':'I', 'LEU':'L', 'ASP':'D', 'GLU':'E', 'LYS':'K',\n",
        "       'ARG':'R', 'SER':'S', 'THR':'T', 'TYR':'Y', 'HIS':'H',\n",
        "       'CYS':'C', 'ASN':'N', 'GLN':'Q', 'TRP':'W', 'GLY':'G',\n",
        "       'MSE':'M', 'UNK':'X',\n",
        "    }\n",
        "    \n",
        "    ca_pattern=re.compile(\"^ATOM\\s{2,6}\\d{1,5}\\s{2}CA\\s[\\sA]([A-Z]{3})\\s([\\s\\w])|^HETATM\\s{0,4}\\d   {1,5}\\s{2}CA\\s[\\sA](MSE)\\s([\\s\\w])\")\n",
        "\n",
        "    filename=os.path.basename(pdb_file).split('.')[0]\n",
        "    dirname=os.path.dirname(pdb_file)\n",
        "    chain_dict=dict()\n",
        "    chain_list=[]\n",
        "\n",
        "    fp=open(pdb_file,'rU')\n",
        "    for line in fp.read().splitlines():\n",
        "        if line.startswith(\"ENDMDL\"):\n",
        "            break\n",
        "        match_list=ca_pattern.findall(line)\n",
        "        if match_list:\n",
        "            resn=match_list[0][0]+match_list[0][2]\n",
        "            chain=match_list[0][1]+match_list[0][3]\n",
        "            if chain in chain_dict:\n",
        "                chain_dict[chain]+=aa3to1[resn]\n",
        "            else:\n",
        "                chain_dict[chain]=aa3to1[resn]\n",
        "                chain_list.append(chain)\n",
        "    fp.close()\n",
        "\n",
        "    for chain in chain_list:\n",
        "\n",
        "      if not os.path.exists(dirname+\"/output/\"+filename+\"/\"):\n",
        "        os.makedirs(os.path.dirname(dirname+\"/output/\"+filename+\"/\"))\n",
        "\n",
        "      fasta_file = open(dirname+\"/output/\"+filename+\"/\"+filename+\".fasta\", \"a\")\n",
        "      fasta_file.write('>%s:%s\\n%s\\n'%(filename,chain,chain_dict[chain]))\n",
        "      fasta_file.close()\n",
        "      shutil.copy(dirname+\"/\"+filename+\".pdb\", dirname+\"/output/\"+filename+\"/\"+filename+\".pdb\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZuflrFORSAt"
      },
      "source": [
        "## Converting all PDB files to FASTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uI0LZJVRCbQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e5554c-d919-4c98-c973-c2a045d9bcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting PDB files to FASTA files and writing to:/content/drive/MyDrive/PDB/output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 29127.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Import Module\n",
        "import os\n",
        "\n",
        "# Change the directory\n",
        "os.chdir(path)\n",
        "\n",
        "# Read text File\n",
        "def read_text_file(file_path):\n",
        "\twith open(file_path, 'r') as f:\n",
        "\t\tprint(f.read())\n",
        "\n",
        "\n",
        "# iterate through all file\n",
        "\n",
        "print(\"Converting PDB files to FASTA files and writing to:\"+ path + \"/output\" )\n",
        "for file in tqdm(os.listdir()):\n",
        "\t# Check whether file is in text format or not\n",
        "\tif file.endswith(\".pdb\"):\n",
        "\t\tfile_path = f\"{path}/{file}\"\n",
        "\t\tconvertPDBtoFASTA(file_path)\n",
        "\t\n",
        "\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMI-MgcRVgd"
      },
      "source": [
        "## Function to get all second sequences in FASTA files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-P_aZfB-BtK0"
      },
      "outputs": [],
      "source": [
        "def get_second_line(file):\n",
        "\twith open(file, 'r') as f:\n",
        "\t\treturn f.readlines()[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RzYhwmx3jWeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9721a5a-ca25-4a21-a556-c992c847f499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting the second sequence in all fasta files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 430/430 [06:24<00:00,  1.12it/s]\n"
          ]
        }
      ],
      "source": [
        "path = path+\"/output\"\n",
        "\n",
        "# Change the directory\n",
        "os.chdir(path)\n",
        "\n",
        "# Read text File\n",
        "def read_text_file(file_path):\n",
        "\twith open(file_path, 'r') as f:\n",
        "\t\tprint(f.read())\n",
        "\n",
        "\n",
        "# iterate through all file\n",
        "ff=[]\n",
        "all_folders = os.listdir()\n",
        "\n",
        "print(\"Extracting the second sequence in all fasta files...\")\n",
        "for folder in tqdm(all_folders):\n",
        "\tos.chdir(path+\"/\"+folder)\n",
        "\tfor file in os.listdir():\n",
        "\t\tif file.endswith(\".fasta\"):\n",
        "\t\t\tfile_path = f\"{path}/{folder}/{file}\"\n",
        "\t\t\tff.append((file,get_second_line(file_path)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2lkcibpKN5G"
      },
      "outputs": [],
      "source": [
        "ff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kys5HTgpSBGo"
      },
      "source": [
        "# ESM-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J09EYhmvR8k3"
      },
      "source": [
        "## Load ESM-2 pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PZdezMlXQuK7"
      },
      "outputs": [],
      "source": [
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9qhiF95UBGS"
      },
      "source": [
        "## *** For Google Colab: Limiting number of proteins to 10\n",
        "\n",
        "Due to memory constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9SrWwkQCUAQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ff7dff-d3e5-40a2-83dd-38cbd326f33b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('3wfd_bundle.fasta',\n",
              "  'YFVFALILFVGQILFGLHTNLLIVWLLFGFMGAAILFWVFAAAGVLTILGPTISKAGIVIVALGFLFLMTGLIGLALLFLFSFWVVHLWVEGVWELIMGAILLYVIIAMALISGIIGTFSALEPLPFFAMVLFAWAMGTTVMAFLGAGVWGFMHGHMAFYGAYAMIVMTEMWGFWLMTVAMVFITLEGAGVVFLIGLVAYLL\\n'),\n",
              " ('4umv_bundle.fasta',\n",
              "  'LITLIVMMAISWGLEQGQLAFIATTLVGLYPIARQALRFAIETLMSVAAIGALFIGTAEAAMVLLLFLISRIYTPAIMAVALLVTLTLLLIGCPCALVISNITIALGLKGIFLVTTWLAVLADTGATVLVTA\\n'),\n",
              " ('4u2p_bundle.fasta', 'MAIVFAYIGVSVVLSARIVAGVWWFFTLIIIFYILVGGLGLAML\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# ff = ff[0:3]\n",
        "\n",
        "ff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF1_8_lxsViZ"
      },
      "source": [
        "## Generate ESM-2 embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "z34KCqz5p8aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f018d8-2c1a-4d6d-cdf2-0fe9825f4fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating protein embeddings using ESM-2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:10<00:00,  3.53s/it]\n"
          ]
        }
      ],
      "source": [
        "from math import e\n",
        "print(\"Generating protein embeddings using ESM-2...\")\n",
        "\n",
        "embeddingsMatrix = []\n",
        "fileList = []\n",
        "\n",
        "for tuple in tqdm(ff):\n",
        "  data = [tuple]\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "  token_representations = results[\"representations\"][33]\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  \n",
        "  index = 0\n",
        "  # file = open(path+\"/\"+data[0][0].split(\".\")[0]+\"/\"+data[0][0].split(\".\")[0]+\"_esm2.txt\", \"w+\")\n",
        "  if len(sequence_representations)< 2:\n",
        "    # torch.save(sequence_representations[0], path+\"/\"+data[0][0].split(\".\")[0]+\"/\"+data[0][0].split(\".\")[0]+\"_esm2.txt\")\n",
        "    np.savetxt(path+\"/\"+data[0][0].split(\".\")[0]+\"/\"+data[0][0].split(\".\")[0]+\"_esm2.txt\", torch.Tensor(sequence_representations[0]).numpy())\n",
        "    # embeddingsMatrix = np.concatenate([embeddingsMatrix, torch.Tensor(sequence_representations[0]).numpy()])\n",
        "    # embeddingsMatrix =  embeddingsMatrix.append(torch.Tensor(sequence_representations[0]).numpy().tolist())\n",
        "\n",
        "    embeddingsMatrix.append(torch.Tensor(sequence_representations[0]).numpy().tolist())\n",
        "    fileList.append(data[0][0])\n",
        "    \n",
        "    # content = str(sequence_representations[0])\n",
        "    # file.write(content)\n",
        "    # file.close()\n",
        "  else:\n",
        "    print(\"more than one representations\")\n",
        "  # Saving the array in a text file\n",
        "  # content = str(sequence_representations)\n",
        "  # file.write(content)\n",
        "  # file.close()\n",
        "\n",
        "embeddingsMatrix = np.array(embeddingsMatrix)\n",
        "\n",
        "# # Look at the unsupervised self-attention map contact predictions\n",
        "# import matplotlib.pyplot as plt\n",
        "# for (_, seq), tokens_len, attention_contacts in zip(ff, batch_lens, results[\"contacts\"]):\n",
        "#     plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
        "#     plt.title(seq)\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "B0OvCZc9rS8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fab303-6c8e-494a-b1c5-4550a2cb5139"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0464871 , -0.02888247,  0.02088749, ..., -0.03872886,\n",
              "         0.05498847, -0.08591463],\n",
              "       [ 0.025186  , -0.05107421,  0.03354084, ...,  0.09429529,\n",
              "        -0.06711466, -0.03865179],\n",
              "       [ 0.02173893,  0.010485  , -0.0038078 , ...,  0.0890759 ,\n",
              "        -0.10418801, -0.09350579]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "embeddingsMatrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileList"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hLy4SOxQOKy",
        "outputId": "be70408e-8e83-491a-e043-62abc7866f70"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3wfd_bundle.fasta', '4umv_bundle.fasta', '4u2p_bundle.fasta']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA on ESM2"
      ],
      "metadata": {
        "id": "lVTHhFq3SV_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import decomposition, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "std_slc = StandardScaler()\n",
        "X_std = std_slc.fit_transform(embeddingsMatrix)\n",
        "\n",
        "print(X_std.shape)\n",
        "print(X_std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AckjqauRZMj",
        "outputId": "7a095b71-7684-40f1-dc60-67d48c549c5d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 1280)\n",
            "[[-1.41292743 -0.22491244  0.25880867 ... -1.41336488  1.37874278\n",
            "  -0.54491381]\n",
            " [ 0.75868494 -1.09670084  1.07465694 ...  0.74910626 -0.41678881\n",
            "   1.40263464]\n",
            " [ 0.6542425   1.32161328 -1.33346561 ...  0.66425862 -0.96195397\n",
            "  -0.85772083]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = decomposition.PCA(n_components=2)\n",
        "\n",
        "X_std_pca = pca.fit_transform(X_std)\n",
        "\n",
        "print(X_std_pca.shape)\n",
        "print(X_std_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5a6gd71RfQK",
        "outputId": "6d1bc494-1e36-4d8b-87d2-2ed55b260ffd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n",
            "[[ 38.60445162  -0.24471149]\n",
            " [-19.05206388  28.44486874]\n",
            " [-19.55238774 -28.20015725]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4OxlK4HB-3x"
      },
      "source": [
        "# AminoBert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqcOMBrvCPZj"
      },
      "source": [
        "## Download RGN2 and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-sb8IMSnqccg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from IPython import get_ipython\n",
        "from IPython.utils import io\n",
        "\n",
        "WORKDIR = './rgn2'\n",
        "GIT_REPO = 'https://github.com/aqlaboratory/rgn2'\n",
        "ENV_CONFIG = os.path.join(WORKDIR, 'environment.yml')\n",
        "RGN2_PARAM_SOURCE_URL = 'https://huggingface.co/christinafl/rgn2'\n",
        "RGN2_PARAMS_DIR = os.path.join(WORKDIR, 'resources')\n",
        "RGN2_PARAM_RUN_DIR = os.path.join(RGN2_PARAMS_DIR, 'rgn2_runs')\n",
        "RGN2_RUN_DIR = os.path.join(WORKDIR, 'runs')\n",
        "\n",
        "AF2_GIT_REPO = 'https://github.com/deepmind/alphafold'\n",
        "AF2_SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_2022-03-02.tar'\n",
        "AF2_PARAMS_DIR = './alphafold/data/params'\n",
        "AF2_PARAMS_PATH = os.path.join(AF2_PARAMS_DIR, os.path.basename(AF2_SOURCE_URL))\n",
        "\n",
        "REFINER_DIR = os.path.join(WORKDIR, 'ter2pdb')\n",
        "REFINER_PATH = os.path.join(REFINER_DIR, 'ModRefiner-l.zip')\n",
        "REFINER_URL = 'https://zhanggroup.org/ModRefiner/ModRefiner-l.zip'\n",
        "\n",
        "try:\n",
        "  with io.capture_output() as captured:\n",
        "    %cd '/content'\n",
        "\n",
        "    # Different conda envs necessary due to conflicting dependencies.\n",
        "    %shell rm -rf /opt/conda\n",
        "    %shell wget -q -P /tmp \\\n",
        "      https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
        "        && bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n",
        "        && rm /tmp/Miniconda3-latest-Linux-x86_64.sh\n",
        "    \n",
        "    PATH=%env PATH\n",
        "    %env PATH=/opt/conda/bin:{PATH}\n",
        "\n",
        "    CONDA_INIT = 'source /opt/conda/etc/profile.d/conda.sh && conda init'\n",
        "    RGN2_ENV_INIT = f'{CONDA_INIT} && conda activate rgn2'\n",
        "\n",
        "    # Download RGN2.\n",
        "    %shell rm -rf {WORKDIR}\n",
        "    %shell git clone {GIT_REPO} {WORKDIR}\n",
        "    %shell {CONDA_INIT} && conda env create -f {ENV_CONFIG}\n",
        "\n",
        "    # Download AF2 for AF2Rank-based refinement.\n",
        "    AF2_ENV_INIT = f'{CONDA_INIT} && conda activate af2'\n",
        "\n",
        "    %shell rm -rf alphafold\n",
        "    %shell git clone --branch main {AF2_GIT_REPO} alphafold\n",
        "    %shell {CONDA_INIT} && conda create -y -q --name af2 python=3.7\n",
        "    %shell {AF2_ENV_INIT} && pip install -r ./alphafold/requirements.txt\n",
        "    %shell {AF2_ENV_INIT} && pip install --no-dependencies ./alphafold\n",
        "    %shell {AF2_ENV_INIT} && pip install --upgrade jax==0.3.17 \\\n",
        "      jaxlib==0.3.15+cuda11.cudnn805 \\\n",
        "      -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "    \n",
        "    %shell mkdir --parents \"{AF2_PARAMS_DIR}\"\n",
        "    %shell wget -O \"{AF2_PARAMS_PATH}\" \"{AF2_SOURCE_URL}\"\n",
        "    %shell tar --extract --verbose --file=\"{AF2_PARAMS_PATH}\" \\\n",
        "      --directory=\"{AF2_PARAMS_DIR}\" --preserve-permissions\n",
        "    %shell rm \"{AF2_PARAMS_PATH}\"\n",
        "\n",
        "    # Download AminoBERT/RGN2 weights.\n",
        "    %shell GIT_LFS_SKIP_SMUDGE=1 git clone \"{RGN2_PARAM_SOURCE_URL}\" \"{RGN2_PARAMS_DIR}\"\n",
        "    %shell cd {RGN2_PARAMS_DIR} && git lfs pull\n",
        "    %shell mv {RGN2_PARAM_RUN_DIR} {RGN2_RUN_DIR}\n",
        "\n",
        "    # Download Modrefiner to initialize all atoms from CA trace.\n",
        "    %shell wget -O {REFINER_PATH} {REFINER_URL}\n",
        "    %shell unzip -o {REFINER_PATH} -d {REFINER_DIR}\n",
        "    %shell rm {REFINER_PATH}\n",
        "\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEc-_UoFCbf_"
      },
      "source": [
        "## Import python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cfN-5qMtCihz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c198d5-ec55-4bf2-9022-abaf76700cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rgn2\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/rgn2'\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import hashlib\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from IPython.utils import io\n",
        "from google.colab import files\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "sys.path.append('/content/alphafold')\n",
        "from ter2pdb import ter2pdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOB150HClwy"
      },
      "source": [
        "## Function to fold amino acid sequence "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Urko2dMODSp9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generateSeqParameters(sequence, jobname):\n",
        "  # Remove whitespace\n",
        "  sequence = \"\".join(sequence.split()).upper()\n",
        "  jobname = \"\".join(jobname.split())\n",
        "\n",
        "  jobname = re.sub(r'\\W+', '', jobname)\n",
        "  seq_hash = hashlib.blake2b(sequence.encode(), digest_size=3).hexdigest()\n",
        "  seq_id = f'{jobname}_{seq_hash}'\n",
        "\n",
        "  MAX_SEQUENCE_LENGTH = 1023\n",
        "\n",
        "  # Remove all whitespaces, tabs and end lines; upper-case\n",
        "  sequence = sequence.translate(str.maketrans('', '', ' \\n\\t')).upper()\n",
        "  aatypes = set('ACDEFGHIKLMNPQRSTVWYX')  # 20 standard aatypes\n",
        "  if not set(sequence).issubset(aatypes):\n",
        "    raise Exception(f'Input sequence contains non-amino acid letters: {set(sequence) - aatypes}. AlphaFold only supports 20 standard amino acids as inputs.')\n",
        "  if len(sequence) > MAX_SEQUENCE_LENGTH:\n",
        "    raise Exception(f'Input sequence is too long: {len(sequence)} amino acids, while the maximum is {MAX_SEQUENCE_LENGTH}. Please use the full AlphaFold system for long sequences.')\n",
        "\n",
        "  run_inputs = {'sequence': sequence, 'seq_id': seq_id}\n",
        "  with open(\"run_\"+jobname+\".json\", \"w\") as f:\n",
        "      json.dump(run_inputs, f)\n",
        "      print(f)\n",
        "  DATA_DIR = 'aminobert_output'\n",
        "  RUN_DIR = 'runs/15106000'\n",
        "  OUTPUT_DIR = 'output'\n",
        "# REFINE_DIR = 'output/refine_model1'\n",
        "# SEQ_PATH = os.path.join(DATA_DIR, f'{seq_id}.fa')\n",
        "# TER_PATH = os.path.join(RUN_DIR, '1', 'outputsTesting', f'{seq_id}.tertiary')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "dSSiSTHIMZQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba594098-cf73-4573-e313-5245fa0da88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_io.TextIOWrapper name='run_3wfd_bundlefasta.json' mode='w' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='run_4umv_bundlefasta.json' mode='w' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='run_4u2p_bundlefasta.json' mode='w' encoding='UTF-8'>\n"
          ]
        }
      ],
      "source": [
        "for f in ff:\n",
        "  generateSeqParameters(f[1], f[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCGkdV51DdQP"
      },
      "source": [
        "## Generate AminoBert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "n-dLN5yHDhCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8b6f0c-a92b-452f-b159-a55f904d304e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no change     /opt/conda/condabin/conda\n",
            "no change     /opt/conda/bin/conda\n",
            "no change     /opt/conda/bin/conda-env\n",
            "no change     /opt/conda/bin/activate\n",
            "no change     /opt/conda/bin/deactivate\n",
            "no change     /opt/conda/etc/profile.d/conda.sh\n",
            "no change     /opt/conda/etc/fish/conf.d/conda.fish\n",
            "no change     /opt/conda/shell/condabin/Conda.psm1\n",
            "no change     /opt/conda/shell/condabin/conda-hook.ps1\n",
            "no change     /opt/conda/lib/python3.10/site-packages/xontrib/conda.xsh\n",
            "no change     /opt/conda/etc/profile.d/conda.csh\n",
            "no change     /root/.bashrc\n",
            "No action taken.\n",
            "['run_3wfd_bundlefasta.json', 'run_4umv_bundlefasta.json', 'run_4u2p_bundlefasta.json']\n",
            "Sequences being removed due to length: 0\n",
            "Sequences being removed: [] []\n",
            "Featurizing input\n",
            "Writing numpy arrays\n",
            "Sequences being removed due to length: 0\n",
            "Sequences being removed: [] []\n",
            "Featurizing input\n",
            "Writing numpy arrays\n",
            "Sequences being removed due to length: 0\n",
            "Sequences being removed: [] []\n",
            "Featurizing input\n",
            "Writing numpy arrays\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/rgn2/aminobert/optimization.py:110: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/prediction.py:18: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fd74c21be60>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "WARNING:tensorflow:From /opt/conda/envs/rgn2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/run_finetuning_and_prediction.py:331: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/modeling.py:174: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/modeling.py:415: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/modeling.py:497: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/modeling.py:678: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /opt/conda/envs/rgn2/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/modeling.py:281: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/run_finetuning_and_prediction.py:280: The name tf.accumulate_n is deprecated. Please use tf.math.accumulate_n instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/run_finetuning_and_prediction.py:315: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/run_finetuning_and_prediction.py:362: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/rgn2/aminobert/run_finetuning_and_prediction.py:377: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "WARNING:tensorflow:From /opt/conda/envs/rgn2/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2023-01-10 05:22:23.244906: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "2023-01-10 05:22:23.250664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2023-01-10 05:22:23.250871: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c426584e00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-10 05:22:23.250903: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-01-10 05:22:23.252736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2023-01-10 05:22:24.026801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:24.027659: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c426585340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-01-10 05:22:24.027693: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-01-10 05:22:24.027892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:24.028505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-10 05:22:24.028837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-01-10 05:22:24.030161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-01-10 05:22:24.031361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-01-10 05:22:24.031709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-01-10 05:22:24.033198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-01-10 05:22:24.034239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-01-10 05:22:24.037153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-01-10 05:22:24.037284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:24.037938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:24.038495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-01-10 05:22:24.038571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-01-10 05:22:24.039801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-01-10 05:22:24.039827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2023-01-10 05:22:24.039838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2023-01-10 05:22:24.039955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:24.040549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:24.041121: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-01-10 05:22:24.041172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2023-01-10 05:22:25.847628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "WARNING:tensorflow:From /content/rgn2/data_processing/aminobert_postprocessing.py:165: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fd74a17bc20>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "2023-01-10 05:22:29.148020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:29.148659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-10 05:22:29.148762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-01-10 05:22:29.148792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-01-10 05:22:29.148813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-01-10 05:22:29.148839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-01-10 05:22:29.148864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-01-10 05:22:29.148888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-01-10 05:22:29.148913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-01-10 05:22:29.149032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:29.149676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:29.150190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-01-10 05:22:29.150263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-01-10 05:22:29.150279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2023-01-10 05:22:29.150288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2023-01-10 05:22:29.150396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:29.150955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:29.151565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fd74c1d24d0>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "2023-01-10 05:22:33.498236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:33.498877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2023-01-10 05:22:33.499006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2023-01-10 05:22:33.499046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2023-01-10 05:22:33.499068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2023-01-10 05:22:33.499088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2023-01-10 05:22:33.499114: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2023-01-10 05:22:33.499146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2023-01-10 05:22:33.499172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-01-10 05:22:33.499272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:33.499912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:33.500445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2023-01-10 05:22:33.500493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-01-10 05:22:33.500511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2023-01-10 05:22:33.500521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2023-01-10 05:22:33.500639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:33.501264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-01-10 05:22:33.501777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "source /opt/conda/etc/profile.d/conda.sh && conda init\n",
        "conda activate rgn2\n",
        "python\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "sys.path.append(os.path.join(os.getcwd(), 'aminobert'))\n",
        "\n",
        "import shutil\n",
        "from aminobert.prediction import aminobert_predict_sequence\n",
        "from data_processing.aminobert_postprocessing import aminobert_postprocess\n",
        "import glob\n",
        "\n",
        "jsonFilenamesList = glob.glob('run_*.json')\n",
        "\n",
        "print(jsonFilenamesList)\n",
        "\n",
        "def generateAminoBertEmbeddings(jobname):\n",
        "  DATA_DIR = 'aminobert_output'+jobname\n",
        "  DATASET_NAME = '1'\n",
        "  PREPEND_M = True\n",
        "  AMINOBERT_CHKPT_DIR = 'resources/aminobert_checkpoint/AminoBERT_runs_v2_uniparc_dataset_v2_5-1024_fresh_start_model.ckpt-1100000'\n",
        "\n",
        "  with open(jobname, \"r\") as f:\n",
        "      run_inputs = json.load(f)\n",
        "\n",
        "  # Remove old data since AminoBERT combines entire directory to create dataset.\n",
        "  if os.path.exists(DATA_DIR):\n",
        "    shutil.rmtree(DATA_DIR)\n",
        "  os.makedirs(DATA_DIR)\n",
        "\n",
        "  aminobert_predict_sequence(seq=run_inputs['sequence'], header=run_inputs['seq_id'],\n",
        "                            prepend_m=PREPEND_M, checkpoint=AMINOBERT_CHKPT_DIR,\n",
        "                            data_dir=DATA_DIR)\n",
        "\n",
        "  aminobert_postprocess(data_dir=DATA_DIR, dataset_name=DATASET_NAME, prepend_m=PREPEND_M)\n",
        "\n",
        "\n",
        "\n",
        "for filename in jsonFilenamesList:\n",
        "  generateAminoBertEmbeddings(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.load('/content/rgn2/aminobert_outputrun_4umv_bundlefasta.json/4umv_bundlefasta_8775ee.fa.npy')\n",
        "\n",
        "arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7oMC4-6IdYK",
        "outputId": "b35adc61-78a9-4894-a46d-a9c115eec764"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.24777073,  0.03763653, -0.24024817, ..., -0.13045666,\n",
              "        -0.14153406, -0.00834489],\n",
              "       [-0.24798127, -0.30669403,  0.2691799 , ..., -0.35812968,\n",
              "        -0.21155265, -0.10475893],\n",
              "       [-0.34240478, -0.1337045 ,  0.4851634 , ..., -0.36997408,\n",
              "        -0.16279061,  0.17824659],\n",
              "       ...,\n",
              "       [-0.14960656,  0.00379328,  0.35912746, ...,  0.07052035,\n",
              "         0.08182491, -0.28427076],\n",
              "       [-0.18910071,  0.01758537,  0.0166251 , ..., -0.14113888,\n",
              "        -0.12628055, -0.1766412 ],\n",
              "       [-0.5913821 , -0.07150503, -0.14718018, ..., -0.02657726,\n",
              "         0.01884503, -0.56912285]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1x-iviE-KrHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}